{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d1f5411-c24c-43da-a051-00f5d89eaab5",
   "metadata": {},
   "source": [
    "## Importowanie bibliotek\n",
    "Biblioteka `morfeusz2` posłuży do lematyzacji, `plwordnet` i Słowosieć do analizy sentymentu. \n",
    "\n",
    ">**Słowosieć** (ang. PlWordNet) – baza danych leksykalno-semantycznych języka polskiego typu wordnet. Zawiera zestawy synonimicznych jednostek leksykalnych (synsety) opisanych krótkimi definicjami. Słowosieć służy jako słownik, w którym pojęcia (synsety) i poszczególne znaczenia wyrazów (jednostki leksykalne) zdefiniowane są poprzez miejsce w sieci wzajemnych relacji, odzwierciedlających system leksykalny polszczyzny. Słowosieć jest także wykorzystywana jako jeden z podstawowych zasobów do budowy programów przetwarzających język polski. ([Wikipedia](https://pl.wikipedia.org/wiki/S%C5%82owosie%C4%87))\n",
    "\n",
    "\n",
    "- http://morfeusz.sgjp.pl/download/\n",
    "- https://slowosiec.pl/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8cfcbd8-07e6-4269-9f94-972499f4cf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plwordnet\n",
    "import morfeusz2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fa962b-6c08-40e3-bdb3-e808582ac982",
   "metadata": {},
   "source": [
    "## Wstępne przetwarzanie tekstu\n",
    "W tym punkcie podejmiemy następujące kroki:\n",
    "1. Usunięcie niepotrzebnych znaków z tekstu źródłowego\n",
    "2. Lematyzacja za pomocą Morfeusza\n",
    "3. Usunięcie stop słów z lematów\n",
    "   \n",
    "Na początku załadujmy polski słownik sentymentu (ręcznie przefiltrowany `plwordnet` w csv). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f236e11a-5edb-4e71-9aa3-037f6690283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "slownik_sentymentu = pd.read_csv('słowniki/SłownikSentymentu.csv', sep=';',encoding='cp1250')\n",
    "# print(slownik_sentymentu.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cacf604-6dfd-4ea7-842e-d698dd170252",
   "metadata": {},
   "source": [
    "Wczytujemy zescrapowane pliki tekstowe z Reddita/Wykopu i dzielimy na pojedyncze słowa. Na samym końcu tworzymy ramkę danych, gdzie każde słowo jest w osobnym wierszu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0822ea61-926d-4412-883b-34c39ac41dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/results_reddit.txt', 'r', encoding='utf-8') as file:\n",
    "    tekst_cały = file.read()\n",
    "slowa = tekst_cały.split()\n",
    "ramka_slow = pd.DataFrame(slowa, columns=['słowa'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2604e4-c916-4e50-90c6-da0d9558e686",
   "metadata": {},
   "source": [
    "Kolejnym krokiem będzie przeczyszczenie wczytanego tekstu, czyli:\n",
    "- usunięcie znaków interpunkcyjnych\n",
    "- usunięcie niepożądanych słów z tekstu źródłowego\n",
    "- usunięcie wszystkich linków z wpisów\n",
    "- usunięcie liczb\n",
    "- usunięcie polskich stop słów\n",
    "  \n",
    "W tym celu korzystamy z prostych list comprehensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67f1d85b-9451-4e3b-b975-91c0f051abf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Znaki interpunkcyjne\n",
    "to_remove = '-,? .>#:\"/[]+=!%<*()|–„'\n",
    "table = str.maketrans('', '', to_remove)\n",
    "cleaned_slowa = [word.translate(table) for word in slowa]\n",
    "\n",
    "# Niechciane słowa\n",
    "remove_slowa = ['Tytuł:','tytuł:', 'Treść:','treść:','Tag:', 'Subreddit:']\n",
    "filtered_words = [word for word in cleaned_slowa if word not in remove_slowa and word != '']\n",
    "# Linki\n",
    "filtered_words_no_links = [\n",
    "    word for word in filtered_words\n",
    "    if not (word.startswith('http') or word.startswith('www'))\n",
    "]\n",
    "# Liczby\n",
    "filtered_words_no_links_no_digits = [\n",
    "    word for word in filtered_words_no_links\n",
    "    if not word.isdigit()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642a23f0-9f6b-4b8c-b084-3ebe78967f00",
   "metadata": {},
   "source": [
    "Na koniec tworzymy ramkę danych z przefiltrowanymi słowami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db5c5cfa-ab3b-41d8-94d3-c4796b3f999d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ramka_slow = pd.DataFrame(filtered_words_no_links_no_digits, columns=['slowa'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c6fe91-fa1f-4e3e-a997-6c2a15145993",
   "metadata": {},
   "source": [
    "### Lematyzacja\n",
    "O Morfeuszu twórcy piszą, że w skrócie to słownik morfologiczny.\n",
    "Matura z polskiego już za nami za równo w wersji podstawowej, jak i rozszerzonej, a z Morfeusza korzystamy w wersji ze słownikiem gramatyki języka polskiego - [SGJP](http://sgjp.pl/o-slowniku/#liczby). \n",
    "\n",
    "**Ważne pojęcia** – czytamy dokumentację twórców\n",
    ">Celem hasłowania (lematyzacji) jest wskazanie dla każdego słowa tekstowego opisującej je jednostki słownika morfologicznego (leksemu). Jest to więc analiza morfologiczna (lub tagowanie) ograniczona tylko do części informacji o formach — do lematów.\n",
    "\n",
    "> Przybliżone hasłowanie polegające na odcięciu ze słów części zmieniającej się przy odmianie bywa nazywane stemowaniem. Metoda ta ma sens dla języków o ograniczonej fleksji, ale dla polskiego daje wyniki wysoce niezadowalające. W kontekście Morfeusza mówimy więc o prawdziwym hasłowaniu.\n",
    "\n",
    "([Analizator morfologiczny Morfeusz](http://morfeusz.sgjp.pl/doc/about/)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf860c88-a88d-4dfb-a36a-dcdee656f2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "morfeusz = morfeusz2.Morfeusz()\n",
    "def lemmatize_word(word):\n",
    "    analyses = morfeusz.analyse(word)\n",
    "    if analyses:\n",
    "        # analyses to lista krotek: (start, end, (lemma, tag), score)\n",
    "        # Bierzemy pierwszą lematę z analizy\n",
    "        lemma = analyses[0][2][1]  # [0] - pierwszy wynik, [2] - tuple (base, lemma, tag), [1] - lemma\n",
    "        return lemma\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "ramka_slow['lemma'] = ramka_slow['slowa'].apply(lemmatize_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee058d3a-6cad-48ca-8e1b-d4e3377de60b",
   "metadata": {},
   "source": [
    "Najczęstsze słowa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f773218-179c-425f-be52-42cb601fdede",
   "metadata": {},
   "outputs": [],
   "source": [
    "ramka_slow['lemma_clean'] = ramka_slow['lemma'].apply(lambda x: x.split(':')[0])\n",
    "lemma_counts = ramka_slow['lemma_clean'].value_counts()\n",
    "# print(lemma_counts.head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5f4ded-57a5-48e5-9e3e-fbfe3736d244",
   "metadata": {},
   "source": [
    "W końcu możemy przeczyścić lematy ze zbędnych słów - stopwords (się, na, i, w, itp...). W pliku `stopwords-pl.txt` znajdują się wszystkie stopsłowa. Pochodzi on z https://github.com/stopwords-iso/stopwords-pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1800895b-1fdd-4541-81dc-d810168898c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_clean\n",
      "prezydent     182\n",
      "treść         105\n",
      "tytuł         104\n",
      "wybory         86\n",
      "kandydat       79\n",
      "oda            60\n",
      "państwo        58\n",
      "partia         53\n",
      "polski         47\n",
      "rok            46\n",
      "chcieć         43\n",
      "głowa          43\n",
      "tenże          42\n",
      "osoba          40\n",
      "móc            40\n",
      "dzień          36\n",
      "polityczny     35\n",
      "swój           35\n",
      "głos           34\n",
      "urząd          34\n",
      "zostać         34\n",
      "władza         34\n",
      "pierwszy       33\n",
      "Polski         33\n",
      "USA            32\n",
      "lato           32\n",
      "Polska         31\n",
      "wyborczy       31\n",
      "czas           30\n",
      "głosować       30\n",
      "kraj           30\n",
      "mówić          29\n",
      "maić           29\n",
      "rząd           28\n",
      "kampania       27\n",
      "siebie         26\n",
      "strona         26\n",
      "brak           26\n",
      "RP             26\n",
      "spotkać        26\n",
      "Trump          26\n",
      "człowiek       26\n",
      "stan           25\n",
      "wiedzieć       25\n",
      "ustawa         25\n",
      "ukraina        25\n",
      "drugi          25\n",
      "wynik          24\n",
      "mamić          24\n",
      "wysoki         23\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "with open('słowniki/stopwords-pl.txt', 'r', encoding='utf-8') as file:\n",
    "    df_polish_stop_words = file.read().splitlines()\n",
    "ramka_slow = ramka_slow[~ramka_slow['lemma_clean'].isin(df_polish_stop_words)]\n",
    "print(ramka_slow['lemma_clean'].value_counts().head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa2486d-24db-4845-8090-b4fbbc936095",
   "metadata": {},
   "source": [
    "## Analiza sentymentu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca058e73-0002-4270-9f19-f2ee6d233000",
   "metadata": {},
   "outputs": [],
   "source": [
    "ramka_slow = ramka_slow.merge(\n",
    "    slownik_sentymentu,\n",
    "    left_on='lemma_clean', \n",
    "    right_on='lemat',\n",
    "    how='left'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
