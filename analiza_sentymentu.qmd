---
title: "𖦹 Echo wyborcze"
author: 
  - name: "Celina Brzywczy"
    url: "https://github.com/CelinaBrz"
  - name: "Dawid Piotrkowski"
    url: "https://github.com/dpiotrkowski"
date: "2025-05-20"  # This will insert the current date
format:
  html:
    theme: journal
    linkcolor: "#367da9"
    toc: true
    toc-location: left
---

## Importowanie bibliotek
Biblioteka `morfeusz2` posłuży do lematyzacji, `plwordnet` i Słowosieć do analizy sentymentu. 

>**Słowosieć** (ang. PlWordNet) – baza danych leksykalno-semantycznych języka polskiego typu wordnet. Zawiera zestawy synonimicznych jednostek leksykalnych (synsety) opisanych krótkimi definicjami. Słowosieć służy jako słownik, w którym pojęcia (synsety) i poszczególne znaczenia wyrazów (jednostki leksykalne) zdefiniowane są poprzez miejsce w sieci wzajemnych relacji, odzwierciedlających system leksykalny polszczyzny. Słowosieć jest także wykorzystywana jako jeden z podstawowych zasobów do budowy programów przetwarzających język polski. ([Wikipedia](https://pl.wikipedia.org/wiki/S%C5%82owosie%C4%87))


- http://morfeusz.sgjp.pl/download/
- https://slowosiec.pl/


```python
# import plwordnet
import morfeusz2
import pandas as pd
import numpy as np
```


```python
# biblioteki do wizualizacji danych
import matplotlib.pyplot as plt
from matplotlib import font_manager
from matplotlib_inline.backend_inline import set_matplotlib_formats
from wordcloud import WordCloud
from wordcloud import ImageColorGenerator
from PIL import Image

set_matplotlib_formats('svg')
```

## Opis danych
Dane pochodzą z Reddita i Wykopu, zostały one zescrapowane przy uwzględnieniu następujących tagów: 

>wybory,prezydent,prezydenckie,debata,prezydencka,
>Zandberg,Biejat,Nawrocki,Trzaskowski,Hołownia,Mentzen,Braun,
>Maciak,Woch,Jakubiak,Senyszyn,Bartoszewicz,Stanowski

[Nasza dokumentacja procesu webscrapingu](https://github.com/dpiotrkowski/PSI-analiza-sentymentu/blob/main/src/README.md)

![](img/loga.png)

## Wstępne przetwarzanie tekstu
W tym punkcie podejmiemy następujące kroki:
1. Usunięcie niepotrzebnych znaków z tekstu źródłowego
2. Autokorekta tekstu
3. Lematyzacja za pomocą Morfeusza
4. Usunięcie stop słów z lematów
   
Na początku załadujmy polski słownik sentymentu (ręcznie przefiltrowany `plwordnet` w csv). 


```python
slownik_sentymentu = pd.read_csv('słowniki/SłownikSentymentu.csv', sep=';',encoding='cp1250')
# print(slownik_sentymentu.columns.tolist())
```

Wczytujemy zescrapowane pliki tekstowe z Reddita/Wykopu i dzielimy na pojedyncze słowa. Na samym końcu tworzymy ramkę danych, gdzie każde słowo jest w osobnym wierszu.


```python
with open('data/results_reddit.txt', 'r', encoding='utf-8') as file:
    reddit_messy = file.read()
with open('data/results_wykop.txt', 'r', encoding='utf-8') as file:
    wykop_messy = file.read()
#slowa_r = tekst_caly_r.split()
#ramka_slow_r = pd.DataFrame(slowa_r, columns=['słowa'])
```

Kolejnym krokiem będzie przeczyszczenie wczytanego tekstu, czyli:
- usunięcie znaków interpunkcyjnych
- usunięcie niepożądanych słów z tekstu źródłowego
- usunięcie wszystkich linków z wpisów
- usunięcie tagów (`#` popularne na Wykopie)
- usunięcie liczb
- usunięcie polskich stop słów
  
W tym celu korzystamy z prostych list comprehensions.

### 1. Czyszczenie tekstu


```python
niechciane_slowa_reddit = ['r/','Tytuł','tytuł', 'Treść','treść','Tag', 'Subreddit','rpolska']
niechciane_slowa_wykop = ['BRAK', 'TREŚCI', 'Tytuł','tytuł', 'Treść','treść','Tag', 'Subreddit']

def text_roomba(text, unwanted_words):
    # Znaki interpunkcyjne
    punctuation = '-,? .>:"/[]+=!%<*|–„( ͡° ͜ʖ ͡°)'    
    table = str.maketrans('', '', punctuation)
    
    words = text.split()
    # Wszystkie słowa z małej litery
    cleaned_words = [word.translate(table).lower() for word in words]  
    # Filtrowanie slow
    filtered_words = [word for word in cleaned_words if word not in unwanted_words and word != '']

    # Usuwanie linkow
    filtered_words_no_links = [
        word for word in filtered_words
        if not (word.startswith('http') or word.startswith('www'))
    ]

    # Usuwanie tagow
    filtered_words_no_tags = [
        word for word in filtered_words_no_links
        if not word.startswith('#')
    ]
    # Usuwanie liczb
    filtered_words_final = [
        word for word in filtered_words_no_tags
        if not word.isdigit()
    ]
    return filtered_words_final

reddit_txt = text_roomba(reddit_messy, niechciane_slowa_reddit)
wykop_txt = text_roomba(wykop_messy, niechciane_slowa_wykop)
```

Na koniec tworzymy ramkę danych z przefiltrowanymi słowami.


```python
df_reddit = pd.DataFrame(reddit_txt, columns=['slowa'])
df_wykop = pd.DataFrame(wykop_txt, columns=['slowa'])
```

### 2. Autokorekta tekstu

### 3. Lematyzacja
O Morfeuszu twórcy piszą, że w skrócie to słownik morfologiczny.
Matura z polskiego już za nami za równo w wersji podstawowej, jak i rozszerzonej, a z Morfeusza korzystamy w wersji ze słownikiem gramatyki języka polskiego - [SGJP](http://sgjp.pl/o-slowniku/#liczby). 

**Ważne pojęcia** – czytamy dokumentację twórców
>Celem hasłowania (lematyzacji) jest wskazanie dla każdego słowa tekstowego opisującej je jednostki słownika morfologicznego (leksemu). Jest to więc analiza morfologiczna (lub tagowanie) ograniczona tylko do części informacji o formach — do lematów.

> Przybliżone hasłowanie polegające na odcięciu ze słów części zmieniającej się przy odmianie bywa nazywane stemowaniem. Metoda ta ma sens dla języków o ograniczonej fleksji, ale dla polskiego daje wyniki wysoce niezadowalające. W kontekście Morfeusza mówimy więc o prawdziwym hasłowaniu.

([Analizator morfologiczny Morfeusz](http://morfeusz.sgjp.pl/doc/about/)) 


```python
morfeusz = morfeusz2.Morfeusz()

def lemmatize_word(word):
    analyses = morfeusz.analyse(word)
    if analyses:
        # analyses to lista krotek: (start, end, (lemma, tag), score)
        # Bierzemy pierwszą lematę z analizy
        lemma = analyses[0][2][1]  # [0] - pierwszy wynik, [2] - tuple (base, lemma, tag), [1] - lemma
        return lemma
    else:
        return word

df_reddit['lemma'] = df_reddit['slowa'].apply(lemmatize_word)
df_wykop['lemma'] = df_wykop['slowa'].apply(lemmatize_word)
```

Najczęstsze słowa


```python
df_reddit['lemma_clean'] = df_reddit['lemma'].apply(lambda x: x.split(':')[0])
lemma_counts_reddit = df_reddit['lemma_clean'].value_counts()

df_wykop['lemma_clean'] = df_wykop['lemma'].apply(lambda x: x.split(':')[0])
lemma_counts_wykop = df_wykop['lemma_clean'].value_counts()
```

### 4. Stopsłowa
W końcu możemy przeczyścić lematy ze zbędnych słów - stopwords (się, na, i, w, itp...). W pliku `stopwords-pl.txt` znajdują się wszystkie stopsłowa. Pochodzi on z https://github.com/stopwords-iso/stopwords-pl


```python
with open('słowniki/stopwords-pl.txt', 'r', encoding='utf-8') as file:
    df_polish_stop_words = file.read().splitlines()

df_reddit = df_reddit[~df_reddit['lemma_clean'].isin(df_polish_stop_words)]
word_freq_reddit = df_reddit['lemma_clean'].value_counts().head(125)

df_wykop = df_wykop[~df_wykop['lemma_clean'].isin(df_polish_stop_words)]
word_freq_wykop = df_wykop['lemma_clean'].value_counts().head(125)
```

## Chmura słów

Wykorzystamy teraz biblioteki do wizualizacji danych `matplotlib` i `worldcloud` do stworzenia chmury słów. W ramce danych `czestosci_slow` nadal występują słowa, które mogłyby zostać usunięte np. *prezydent*, *kandydat*, *wybory*.

Tworząc chmurę słów za pomocą biblioteki [`wordcloud`](https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html) można dodać stopsłowa, dodając argument `stopwords`. Jednak nie zdecydowaliśmy się na to, ponieważ usunięcie stopsłów przyda się nam również przy analizie sentymentu.



```python
words_to_remove = ['polski', 'polska', 'kandydat', 'prezydent','tenże','wybory', 'oda']
filtered_word_freq_reddit = {word: freq for word, freq in word_freq_reddit.items() if word not in words_to_remove}
filtered_word_freq_wykop = {word: freq for word, freq in word_freq_wykop.items() if word not in words_to_remove}
```


```python
font_path = 'img/RedditSans.ttf'
font_prop = font_manager.FontProperties(fname=font_path)
mask_path = 'img/mask-reddit.png'  
mask = Image.open(mask_path)
mask = mask.convert("L")  # Convert to grayscale
mask = np.array(mask)

wc = WordCloud(font_path = font_path,
               mask=mask,
               width=1920, height=1080,
               background_color = "white",
               colormap = "magma",
               contour_width=12,
               contour_color='#ff4500')
wc.generate_from_frequencies(filtered_word_freq_reddit)

plt.figure(figsize=(14,11),dpi=300)
plt.axis('off')
plt.imshow(wc, interpolation="bilinear")
plt.title('Reddit', color='#ff4500', fontsize=15)
plt.text(0.5, 0.0, 'reddit.com/r/Polska', ha='center', va='center', fontsize=12, color='gray', transform=plt.gca().transAxes)
plt.show()
```


    
![svg](analiza_sentymentu_files/analiza_sentymentu_23_0.svg)
    



```python
mask_path = 'img/mask-wykop.png'  
mask = Image.open(mask_path)
mask = mask.convert("L")  # Convert to grayscale
mask = np.array(mask)


def orange_center_color_func(word, font_size, position, orientation, random_state=None, **kwargs):
    # Get the frequency of the word
    frequency = wc.words_.get(word, 0)
    
    # Assign orange to the most frequent words and blue to others
    if frequency > 0.3:  # Adjust this threshold based on your needs
        return "orange"
    else:
        return "blue"


wc = WordCloud(font_path = font_path,
               mask=mask,
               width=1920, height=1080,
               background_color = "white",
               color_func=orange_center_color_func,
               #colormap = "magma",
               contour_width=30,
               contour_color='#0054a2')
wc.generate_from_frequencies(filtered_word_freq_wykop)

plt.figure(figsize=(13,10),dpi=300)
plt.axis('off')
plt.imshow(wc, interpolation="bilinear")
plt.title('Wykop', color='#0054a2', fontsize=15)
plt.text(0.5, 0.0, 'wykop.pl', ha='center', va='center', fontsize=12, color='gray', transform=plt.gca().transAxes)
plt.show()
```


    
![svg](analiza_sentymentu_files/analiza_sentymentu_24_0.svg)
    


## Analiza sentymentu
Przechodzimy do właściwej analizy sentymentu.


```python
# Dołączenie słownika sentymentu
df_reddit = df_reddit.merge(
    slownik_sentymentu,
    left_on='lemma_clean', 
    right_on='lemat',
    how='left'
)
df_wykop = df_wykop.merge(
    slownik_sentymentu,
    left_on='lemma_clean', 
    right_on='lemat',
    how='left'
)

# Zliczenie ilości słów o danym ładunku emocjonalnym
sentyment_suma_reddit = df_reddit.groupby('nachechowanie').size().reset_index(name='ilosc')
sentyment_suma_wykop = df_wykop.groupby('nachechowanie').size().reset_index(name='ilosc')

# Pominięcie słów bez przypisanego ładunku
sentyment_suma_reddit = sentyment_suma_reddit[sentyment_suma_reddit['nachechowanie'].notna()]
sentyment_suma_wykop = sentyment_suma_wykop[sentyment_suma_wykop['nachechowanie'].notna()]
```

Dołączamy słownik ładunków emocjonalnych do ostatecznie uzyskanej ramki danych i zliczamy wszystkie słowa o danym ładunku.


```python

# Ustalenie kolejności argumentów
custom_order = ['+ m', '+ s', 'amb', '- s', '- m']
sentyment_suma_reddit['nachechowanie'] = pd.Categorical(
    sentyment_suma_reddit['nachechowanie'],
    categories=custom_order,
    ordered=True
)
sentyment_suma_wykop['nachechowanie'] = pd.Categorical(
    sentyment_suma_wykop['nachechowanie'],
    categories=custom_order,
    ordered=True
)

sentyment_suma_reddit = sentyment_suma_reddit.sort_values('nachechowanie', ascending=False)
sentyment_suma_wykop = sentyment_suma_wykop.sort_values('nachechowanie', ascending=False)

# Mapa kolorów dla skali emocji
color_map = {
    '+ m': '#2ecc40',   # Zielony
    '+ s': '#a3e048',   # Jasnozielony
    'amb': '#ffd700',   # Żółty
    '- s': '#ff8c00',   # Pomarańczowy
    '- m': '#e74c3c'    # Czerwony
}
sentyment_suma_reddit['color'] = sentyment_suma_reddit['nachechowanie'].map(color_map)
sentyment_suma_wykop['color'] = sentyment_suma_wykop['nachechowanie'].map(color_map)

# Mapa opisowych nazw dla skali emocji
label_map = {
    '+ m': 'bardzo pozytywny',
    '+ s': 'pozytywny',
    'amb': 'niejednoznaczny',
    '- s': 'negatywny',
    '- m': 'bardzo negatywny'
}
```

 Rezultat zostaje przedstawiony na wykresie lizakowym za pomocą biblioteki `matplotlib`.


```python
# Nowa kolumna z opisowymi nazwami do wyświetlenia na osi Y
sentyment_suma_reddit['label'] = sentyment_suma_reddit['nachechowanie'].map(label_map)
plt.figure(figsize=(10, 6))

# Wykres
for idx, row in sentyment_suma_reddit.iterrows():
    plt.hlines(
        y=row['label'],
        xmin=0,
        xmax=row['ilosc'],
        color=row['color'],
        linewidth=3
    )
    plt.plot(
        row['ilosc'],
        row['label'],
        'o',
        markersize=10,
        color=row['color']
    )
    plt.text(
        row['ilosc'] + max(sentyment_suma_reddit['ilosc']) * 0.02,
        row['label'],
        str(row['ilosc']),
        va='center',
        ha='left',
        fontsize=14,
        color='black'
    )

plt.title('Analiza sentymentu (Reddit)', fontsize=18, pad=15)
plt.xlabel('Liczba obserwacji', labelpad=15, fontsize = 14)
plt.ylabel('Ładunek emocjonalny', labelpad=15, fontsize = 14)
plt.grid(axis='x', linestyle='--', alpha=0.7)
max_ilosc = sentyment_suma_reddit['ilosc'].max()
plt.xlim(0, max_ilosc * 1.15)
plt.tight_layout()
plt.show()
```


    
![svg](analiza_sentymentu_files/analiza_sentymentu_30_0.svg)
    



```python
# Nowa kolumna z opisowymi nazwami do wyświetlenia na osi Y
sentyment_suma_wykop['label'] = sentyment_suma_wykop['nachechowanie'].map(label_map)
plt.figure(figsize=(10, 6))

# Wykres
for idx, row in sentyment_suma_wykop.iterrows():
    plt.hlines(
        y=row['label'],
        xmin=0,
        xmax=row['ilosc'],
        color=row['color'],
        linewidth=3
    )
    plt.plot(
        row['ilosc'],
        row['label'],
        'o',
        markersize=10,
        color=row['color']
    )
    plt.text(
        row['ilosc'] + max(sentyment_suma_wykop['ilosc']) * 0.02,
        row['label'],
        str(row['ilosc']),
        va='center',
        ha='left',
        fontsize=14,
        color='black'
    )

plt.title('Analiza sentymentu (Wykop)', fontsize=18, pad=15)
plt.xlabel('Liczba obserwacji', labelpad=15, fontsize = 14)
plt.ylabel('Ładunek emocjonalny', labelpad=15, fontsize = 14)
plt.grid(axis='x', linestyle='--', alpha=0.7)
max_ilosc = sentyment_suma_reddit['ilosc'].max()
plt.xlim(0, max_ilosc * 1.15)
plt.tight_layout()
plt.show()
```


    
![svg](analiza_sentymentu_files/analiza_sentymentu_31_0.svg)
    


## Wystąpienia poszczególnych kandydatów
Teraz skupimy się bezpośrednio na kwestii obecnych wyborów. Najpierw zliczymy ile razy wspomniano poszczególnych kandydatów, zliczając wystąpienia ich nazwisk odmienionych przez wszystkie przypadki

Minus tej metody: Internauci uwielbiają nadawać ksywki, których tutaj nie uwzględniamy.


```python
df_kandydaci_r = df_reddit[['slowa']]
slownik_kandydaci = pd.read_csv('słowniki/slownik_kandydaci.csv', sep=',')
df_kandydaci_r = df_kandydaci_r.merge(
    slownik_kandydaci,
    left_on='slowa', 
    right_on='odmieniony',
    how='left'
)

count_kandydaci_r = pd.DataFrame({'Kandydaci': ['Trzaskowski', 'Nawrocki', 'Mentzen', 'Hołownia','Biejat',
 'Zandberg','Braun','Stanowski','Senyszyn','Jakubiak','Bartoszewicz','Woch','Maciak']})
Wystąpienia = df_kandydaci_r['mianownik'].value_counts()
count_kandydaci_r['Wystąpienia'] = count_kandydaci_r['Kandydaci'].map(Wystąpienia).fillna(0).astype(int)
kandydaci_posortowani_r = count_kandydaci_r.sort_values(by='Wystąpienia', ascending=False)
#print('Reddit:')
#print(kandydaci_posortowani_r)

df_kandydaci_w = df_wykop[['slowa']]
slownik_kandydaci = pd.read_csv('słowniki/slownik_kandydaci.csv', sep=',')
df_kandydaci_w = df_kandydaci_w.merge(
    slownik_kandydaci,
    left_on='slowa', 
    right_on='odmieniony',
    how='left'
)

count_kandydaci_w = pd.DataFrame({'Kandydaci': ['Trzaskowski', 'Nawrocki', 'Mentzen', 'Hołownia','Biejat',
 'Zandberg','Braun','Stanowski','Senyszyn','Jakubiak','Bartoszewicz','Woch','Maciak']})
Wystąpienia = df_kandydaci_w['mianownik'].value_counts()
count_kandydaci_w['Wystąpienia'] = count_kandydaci_w['Kandydaci'].map(Wystąpienia).fillna(0).astype(int)
kandydaci_posortowani_w = count_kandydaci_w.sort_values(by='Wystąpienia', ascending=False)
#print('Wykop:')
#print(kandydaci_posortowani_w)
print(type(count_kandydaci_r))
```

    <class 'pandas.core.frame.DataFrame'>
    

## Wykres częstości wystąpień nazwisk kandydatów


```python
# tu będzie kod wykresu
count_kandydaci_r.plot.bar(x='Kandydaci', y='Wystąpienia', rot=0, width=0.9, color = 'darkorange', 
title = "Ile razy wspomniano kandydatów (Reddit)")
plt.xticks(rotation=70)                   
plt.tight_layout()                        
plt.show()
```


    
![svg](analiza_sentymentu_files/analiza_sentymentu_35_0.svg)
    



```python
count_kandydaci_w.plot.bar(x='Kandydaci', y='Wystąpienia', rot=0, width=0.9, color = 'blue', 
title = "Ile razy wspomniano kandydatów (Wykop)")
plt.xticks(rotation=70)                   
plt.tight_layout()                        
plt.show()
```


    
![svg](analiza_sentymentu_files/analiza_sentymentu_36_0.svg)
    


## Asocjacje dla nazwisk kandydatów
Tym razem tekst musi zostać podzielony na całe wypowiedzi, a nie słowa.


```python
import re
with open('data/results_reddit.txt', 'r', encoding='utf-8') as file:
    tekst_asoc_r = file.read()

# Usuwanie listy wyrazow
words_to_remove = ['r/Polska', 'polski', 'polska', 'kandydat', 'prezydent','tenże','wybory', 'oda','sonda']
pattern_words = r'\b(?:' + '|'.join(words_to_remove) + r')\b'

zmieniane = [
    # Zamiana "?" i "!" na kropki
    (r'[?!]', '.'),          
    # Usuwanie linków i hashtagów         
    (r'\bhttps?:\S*|#\w+|' + pattern_words, ''),
    # Usuwanie znaków specjalnych i liczb       
    (r'[{}]|\d+'.format(re.escape("''\"*,.:;>[]()%|<+=")), ''),
    # Zamiana / na spację
    (r'/', ' '),
    # Usuwanie wielokrotnych spacji                      
    (r'\s+', ' ')                     
]
for pattern, replacement in zmieniane:
    tekst_asoc_r = re.sub(pattern, replacement, tekst_asoc_r, flags=re.IGNORECASE)

tekst_asoc_r = tekst_asoc_r.strip()
wypowiedz_asoc_r = re.split(r'\s*-{50}\s*', tekst_asoc_r)
```

Następnie dokonujemy lematyzacji i usunięcia stopwords z zachowaniem podziału na zdania.


```python
# lematyzacja
morfeusz = morfeusz2.Morfeusz()

# Funkcja do lematyzacji pojedynczego słowa
def lemmatize(word: str) -> str:
    try:
        analysis = morfeusz.analyse(word)
        if analysis:
            return analysis[0][2][1].split(':')[0]
        return word
    except:
        return word

# Lematyzacja zdań z zachowaniem struktury
asoc_lem_r = []
for wypowiedz in wypowiedz_asoc_r:
    # Dzielimy wypowiedz na słowa, lematyzujemy i łączymy z powrotem
    lematyzowane_slowa = [lemmatize(slowo) for slowo in wypowiedz.split()]
    asoc_lem_r.append(' '.join(lematyzowane_slowa))

# Usuwanie stopwords
def usun_stopwords(sentence, stopwords):
    words = sentence.split()
    filtered_words = [word for word in words if word not in stopwords]
    return ' '.join(filtered_words)

asoc_lem_sw_r = [usun_stopwords(zdanie, df_polish_stop_words) for zdanie in asoc_lem_r]
#ostateczne wyczyszczenie z - i spacji
def clean_text_list(text_list):
    cleaned_list = []
    for text in text_list:
        text = re.sub(r'[„–”-]', '', text)
        text = re.sub(r'\s{2,}', ' ', text)
        text = text.strip()
        cleaned_list.append(text)
    return cleaned_list

asoc_lem_sw_c_r = clean_text_list(asoc_lem_sw_r)
# Zamiana na lowercase
asoc_gotowe_r = [zdanie.lower() for zdanie in asoc_lem_sw_c_r]
```

Macierz częstości słów


```python
from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(asoc_gotowe_r)
df_asoc_r = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())
macierz_korelacji_r = df_asoc_r.corr(method='pearson')
# Funkcja znajdująca słowa najbardziej skorelowane z podanym słowem
def findAssocs(correlation_matrix, term, cor_limit=0.5):      
    if term not in correlation_matrix.columns:
        return f"Słowo '{term}' nie występuje"

    correlations = correlation_matrix[term]
    high_correlations = correlations[(correlations >= cor_limit) & 
                                   (correlations.index != term)]
    # Zwracane jest series z posortowanymi wartościami korelacji powyżej podanego progu
    return high_correlations.sort_values(ascending=False)
```

    <class 'pandas.core.series.Series'>
    

Wykresy asocjacji dla Reddita


```python
def asocjator1000(macierz, nazwisko, sila):
    # Pobierz korelacje dla danego słowa
    nazwisko_corr = findAssocs(macierz, nazwisko, sila)
   
    if isinstance(nazwisko_corr, str):
        print(nazwisko_corr)
        return
    if len(nazwisko_corr) == 0:
        print(f"Nie znaleziono słów z korelacją wyższą niż {sila} dla '{nazwisko}'")
        return
    bar_height = 0.35
    fig_height = len(nazwisko_corr) * bar_height + 1.5
    fig, ax = plt.subplots(figsize=(8, fig_height))
    bars = ax.barh(
        y=nazwisko_corr.index,
        width=nazwisko_corr.values,
        height=bar_height,           
        color='darkorange',
        edgecolor='darkred',
        alpha=0.8
    )
    for bar in bars:
        width = bar.get_width()
        ax.text(
            width + 0.01,
            bar.get_y() + bar.get_height()/2,
            f'{width:.3f}',
            va='center',
            fontsize=8
        )
   
    ax.grid(True, axis='x', linestyle='--', alpha=0.7)
    ax.invert_yaxis()
    ax.set_ylim(len(nazwisko_corr) - 0.5, -0.5)
    ax.tick_params(axis='y', labelsize=9)
    ax.tick_params(axis='x', labelsize=9)
    ax.set_title(f'Asocjacje dla: {nazwisko}', fontsize=12)
    ax.set_xlabel('Wartość korelacji', fontsize=10)
    plt.tight_layout(pad=0.5)
# Trzaskowski
asocjator1000(macierz_korelacji_r,'trzaskowski',0.7)
# Nawrocki
asocjator1000(macierz_korelacji_r,'nawrocki',0.8)
# Mentzen
asocjator1000(macierz_korelacji_r,'mentzen',0.8)
# Hołownia
asocjator1000(macierz_korelacji_r,'hołownia',0.8)
# Biejat
asocjator1000(macierz_korelacji_r,'biejat',0.8)
# Zandberg
asocjator1000(macierz_korelacji_r,'zandberg',0.8)
# Braun
asocjator1000(macierz_korelacji_r,'braun',0.8)
# Stanowski
asocjator1000(macierz_korelacji_r,'stanowski',0.8)
# Senyszyn
asocjator1000(macierz_korelacji_r,'senyszyn',0.8)
# Jakubiak
asocjator1000(macierz_korelacji_r,'jakubiak',0.8)
# Bartoszewicz
asocjator1000(macierz_korelacji_r,'bartoszewicz',0.8)
# Maciak
asocjator1000(macierz_korelacji_r,'maciak',0.8)
# Woch
asocjator1000(macierz_korelacji_r,'woch',0.8)
```


    
![svg](analiza_sentymentu_files/analiza_sentymentu_44_0.svg)
    



    
![svg](analiza_sentymentu_files/analiza_sentymentu_44_1.svg)
    



    
![svg](analiza_sentymentu_files/analiza_sentymentu_44_2.svg)
    



    
![svg](analiza_sentymentu_files/analiza_sentymentu_44_3.svg)
    



    
![svg](analiza_sentymentu_files/analiza_sentymentu_44_4.svg)
    



    
![svg](analiza_sentymentu_files/analiza_sentymentu_44_5.svg)
    



    
![svg](analiza_sentymentu_files/analiza_sentymentu_44_6.svg)
    



    
![svg](analiza_sentymentu_files/analiza_sentymentu_44_7.svg)
    



    
![svg](analiza_sentymentu_files/analiza_sentymentu_44_8.svg)
    



    
![svg](analiza_sentymentu_files/analiza_sentymentu_44_9.svg)
    



    
![svg](analiza_sentymentu_files/analiza_sentymentu_44_10.svg)
    



    
![svg](analiza_sentymentu_files/analiza_sentymentu_44_11.svg)
    



    
![svg](analiza_sentymentu_files/analiza_sentymentu_44_12.svg)
    



```python

```
