---
title: "ñ¶π Echo wyborcze"
author: 
  - name: "Celina Brzywczy"
    url: "https://github.com/CelinaBrz"
  - name: "Dawid Piotrkowski"
    url: "https://github.com/dpiotrkowski"
date: "2025-05-20"  # This will insert the current date
format:
  html:
    theme: journal
    linkcolor: "#367da9"
    toc: true
    toc-location: left
---

## Importowanie bibliotek
Biblioteka `morfeusz2` pos≈Çu≈ºy do lematyzacji, `plwordnet` i S≈Çowosieƒá do analizy sentymentu. 

>**S≈Çowosieƒá** (ang. PlWordNet) ‚Äì baza danych leksykalno-semantycznych jƒôzyka polskiego typu wordnet. Zawiera zestawy synonimicznych jednostek leksykalnych (synsety) opisanych kr√≥tkimi definicjami. S≈Çowosieƒá s≈Çu≈ºy jako s≈Çownik, w kt√≥rym pojƒôcia (synsety) i poszczeg√≥lne znaczenia wyraz√≥w (jednostki leksykalne) zdefiniowane sƒÖ poprzez miejsce w sieci wzajemnych relacji, odzwierciedlajƒÖcych system leksykalny polszczyzny. S≈Çowosieƒá jest tak≈ºe wykorzystywana jako jeden z podstawowych zasob√≥w do budowy program√≥w przetwarzajƒÖcych jƒôzyk polski. ([Wikipedia](https://pl.wikipedia.org/wiki/S%C5%82owosie%C4%87))


- http://morfeusz.sgjp.pl/download/
- https://slowosiec.pl/


```python
# import plwordnet
import morfeusz2
import pandas as pd
import numpy as np
```


```python
# biblioteki do wizualizacji danych
import matplotlib.pyplot as plt
from matplotlib import font_manager
from matplotlib_inline.backend_inline import set_matplotlib_formats
from wordcloud import WordCloud
from wordcloud import ImageColorGenerator
from PIL import Image

set_matplotlib_formats('svg')
```

## Opis danych
Dane pochodzƒÖ z Reddita i Wykopu, zosta≈Çy one zescrapowane przy uwzglƒôdnieniu nastƒôpujƒÖcych tag√≥w: 

>wybory,prezydent,prezydenckie,debata,prezydencka,
>Zandberg,Biejat,Nawrocki,Trzaskowski,Ho≈Çownia,Mentzen,Braun,
>Maciak,Woch,Jakubiak,Senyszyn,Bartoszewicz,Stanowski

[Nasza dokumentacja procesu webscrapingu](https://github.com/dpiotrkowski/PSI-analiza-sentymentu/blob/main/src/README.md)

![](img/loga.png)

## Wstƒôpne przetwarzanie tekstu
W tym punkcie podejmiemy nastƒôpujƒÖce kroki:
1. Usuniƒôcie niepotrzebnych znak√≥w z tekstu ≈∫r√≥d≈Çowego
2. Autokorekta tekstu
3. Lematyzacja za pomocƒÖ Morfeusza
4. Usuniƒôcie stop s≈Ç√≥w z lemat√≥w
   
Na poczƒÖtku za≈Çadujmy polski s≈Çownik sentymentu (rƒôcznie przefiltrowany `plwordnet` w csv). 


```python
slownik_sentymentu = pd.read_csv('s≈Çowniki/S≈ÇownikSentymentu.csv', sep=';',encoding='cp1250')
# print(slownik_sentymentu.columns.tolist())
```

Wczytujemy zescrapowane pliki tekstowe z Reddita/Wykopu i dzielimy na pojedyncze s≈Çowa. Na samym ko≈Ñcu tworzymy ramkƒô danych, gdzie ka≈ºde s≈Çowo jest w osobnym wierszu.


```python
with open('data/results_reddit.txt', 'r', encoding='utf-8') as file:
    reddit_messy = file.read()
with open('data/results_wykop.txt', 'r', encoding='utf-8') as file:
    wykop_messy = file.read()
#slowa_r = tekst_caly_r.split()
#ramka_slow_r = pd.DataFrame(slowa_r, columns=['s≈Çowa'])
```

Kolejnym krokiem bƒôdzie przeczyszczenie wczytanego tekstu, czyli:
- usuniƒôcie znak√≥w interpunkcyjnych
- usuniƒôcie niepo≈ºƒÖdanych s≈Ç√≥w z tekstu ≈∫r√≥d≈Çowego
- usuniƒôcie wszystkich link√≥w z wpis√≥w
- usuniƒôcie tag√≥w (`#` popularne na Wykopie)
- usuniƒôcie liczb
- usuniƒôcie polskich stop s≈Ç√≥w
  
W tym celu korzystamy z prostych list comprehensions.

### 1. Czyszczenie tekstu


```python
niechciane_slowa_reddit = ['r/','Tytu≈Ç','tytu≈Ç', 'Tre≈õƒá','tre≈õƒá','Tag', 'Subreddit','rpolska']
niechciane_slowa_wykop = ['BRAK', 'TRE≈öCI', 'Tytu≈Ç','tytu≈Ç', 'Tre≈õƒá','tre≈õƒá','Tag', 'Subreddit']

def text_roomba(text, unwanted_words):
    # Znaki interpunkcyjne
    punctuation = '-,? .>:"/[]+=!%<*|‚Äì‚Äû( Õ°¬∞ Õú ñ Õ°¬∞)'    
    table = str.maketrans('', '', punctuation)
    
    words = text.split()
    # Wszystkie s≈Çowa z ma≈Çej litery
    cleaned_words = [word.translate(table).lower() for word in words]  
    # Filtrowanie slow
    filtered_words = [word for word in cleaned_words if word not in unwanted_words and word != '']

    # Usuwanie linkow
    filtered_words_no_links = [
        word for word in filtered_words
        if not (word.startswith('http') or word.startswith('www'))
    ]

    # Usuwanie tagow
    filtered_words_no_tags = [
        word for word in filtered_words_no_links
        if not word.startswith('#')
    ]
    # Usuwanie liczb
    filtered_words_final = [
        word for word in filtered_words_no_tags
        if not word.isdigit()
    ]
    return filtered_words_final

reddit_txt = text_roomba(reddit_messy, niechciane_slowa_reddit)
wykop_txt = text_roomba(wykop_messy, niechciane_slowa_wykop)
```

Na koniec tworzymy ramkƒô danych z przefiltrowanymi s≈Çowami.


```python
df_reddit = pd.DataFrame(reddit_txt, columns=['slowa'])
df_wykop = pd.DataFrame(wykop_txt, columns=['slowa'])
```

### 2. Autokorekta tekstu

### 3. Lematyzacja
O Morfeuszu tw√≥rcy piszƒÖ, ≈ºe w skr√≥cie to s≈Çownik morfologiczny.
Matura z polskiego ju≈º za nami za r√≥wno w wersji podstawowej, jak i rozszerzonej, a z Morfeusza korzystamy w wersji ze s≈Çownikiem gramatyki jƒôzyka polskiego - [SGJP](http://sgjp.pl/o-slowniku/#liczby). 

**Wa≈ºne pojƒôcia** ‚Äì czytamy dokumentacjƒô tw√≥rc√≥w
>Celem has≈Çowania (lematyzacji) jest wskazanie dla ka≈ºdego s≈Çowa tekstowego opisujƒÖcej je jednostki s≈Çownika morfologicznego (leksemu). Jest to wiƒôc analiza morfologiczna (lub tagowanie) ograniczona tylko do czƒô≈õci informacji o formach ‚Äî do lemat√≥w.

> Przybli≈ºone has≈Çowanie polegajƒÖce na odciƒôciu ze s≈Ç√≥w czƒô≈õci zmieniajƒÖcej siƒô przy odmianie bywa nazywane stemowaniem. Metoda ta ma sens dla jƒôzyk√≥w o ograniczonej fleksji, ale dla polskiego daje wyniki wysoce niezadowalajƒÖce. W kontek≈õcie Morfeusza m√≥wimy wiƒôc o prawdziwym has≈Çowaniu.

([Analizator morfologiczny Morfeusz](http://morfeusz.sgjp.pl/doc/about/)) 


```python
morfeusz = morfeusz2.Morfeusz()

def lemmatize_word(word):
    analyses = morfeusz.analyse(word)
    if analyses:
        # analyses to lista krotek: (start, end, (lemma, tag), score)
        # Bierzemy pierwszƒÖ lematƒô z analizy
        lemma = analyses[0][2][1]  # [0] - pierwszy wynik, [2] - tuple (base, lemma, tag), [1] - lemma
        return lemma
    else:
        return word

df_reddit['lemma'] = df_reddit['slowa'].apply(lemmatize_word)
df_wykop['lemma'] = df_wykop['slowa'].apply(lemmatize_word)
```

Najczƒôstsze s≈Çowa


```python
df_reddit['lemma_clean'] = df_reddit['lemma'].apply(lambda x: x.split(':')[0])
lemma_counts_reddit = df_reddit['lemma_clean'].value_counts()

df_wykop['lemma_clean'] = df_wykop['lemma'].apply(lambda x: x.split(':')[0])
lemma_counts_wykop = df_wykop['lemma_clean'].value_counts()
```

### 4. Stops≈Çowa
W ko≈Ñcu mo≈ºemy przeczy≈õciƒá lematy ze zbƒôdnych s≈Ç√≥w - stopwords (siƒô, na, i, w, itp...). W pliku `stopwords-pl.txt` znajdujƒÖ siƒô wszystkie stops≈Çowa. Pochodzi on z https://github.com/stopwords-iso/stopwords-pl


```python
with open('s≈Çowniki/stopwords-pl.txt', 'r', encoding='utf-8') as file:
    df_polish_stop_words = file.read().splitlines()

df_reddit = df_reddit[~df_reddit['lemma_clean'].isin(df_polish_stop_words)]
word_freq_reddit = df_reddit['lemma_clean'].value_counts().head(125)

df_wykop = df_wykop[~df_wykop['lemma_clean'].isin(df_polish_stop_words)]
word_freq_wykop = df_wykop['lemma_clean'].value_counts().head(125)
```

## Chmura s≈Ç√≥w

Wykorzystamy teraz biblioteki do wizualizacji danych `matplotlib` i `worldcloud` do stworzenia chmury s≈Ç√≥w. W ramce danych `czestosci_slow` nadal wystƒôpujƒÖ s≈Çowa, kt√≥re mog≈Çyby zostaƒá usuniƒôte np. *prezydent*, *kandydat*, *wybory*.

TworzƒÖc chmurƒô s≈Ç√≥w za pomocƒÖ biblioteki [`wordcloud`](https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html) mo≈ºna dodaƒá stops≈Çowa, dodajƒÖc argument `stopwords`. Jednak nie zdecydowali≈õmy siƒô na to, poniewa≈º usuniƒôcie stops≈Ç√≥w przyda siƒô nam r√≥wnie≈º przy analizie sentymentu.



```python
words_to_remove = ['polski', 'polska', 'kandydat', 'prezydent','ten≈ºe','wybory', 'oda']
filtered_word_freq_reddit = {word: freq for word, freq in word_freq_reddit.items() if word not in words_to_remove}
filtered_word_freq_wykop = {word: freq for word, freq in word_freq_wykop.items() if word not in words_to_remove}
```


```python
font_path = 'img/RedditSans.ttf'
font_prop = font_manager.FontProperties(fname=font_path)
mask_path = 'img/mask-reddit.png'  
mask = Image.open(mask_path)
mask = mask.convert("L")  # Convert to grayscale
mask = np.array(mask)

wc = WordCloud(font_path = font_path,
               mask=mask,
               width=1920, height=1080,
               background_color = "white",
               colormap = "magma",
               contour_width=12,
               contour_color='#ff4500')
wc.generate_from_frequencies(filtered_word_freq_reddit)

plt.figure(figsize=(14,11),dpi=300)
plt.axis('off')
plt.imshow(wc, interpolation="bilinear")
plt.title('Reddit', color='#ff4500', fontsize=15)
plt.text(0.5, 0.0, 'reddit.com/r/Polska', ha='center', va='center', fontsize=12, color='gray', transform=plt.gca().transAxes)
plt.show()
```


    
![svg](analiza_sentymentu_files/analiza_sentymentu_23_0.svg)
    



```python
mask_path = 'img/mask-wykop.png'  
mask = Image.open(mask_path)
mask = mask.convert("L")  # Convert to grayscale
mask = np.array(mask)


def orange_center_color_func(word, font_size, position, orientation, random_state=None, **kwargs):
    # Get the frequency of the word
    frequency = wc.words_.get(word, 0)
    
    # Assign orange to the most frequent words and blue to others
    if frequency > 0.3:  # Adjust this threshold based on your needs
        return "orange"
    else:
        return "blue"


wc = WordCloud(font_path = font_path,
               mask=mask,
               width=1920, height=1080,
               background_color = "white",
               color_func=orange_center_color_func,
               #colormap = "magma",
               contour_width=30,
               contour_color='#0054a2')
wc.generate_from_frequencies(filtered_word_freq_wykop)

plt.figure(figsize=(13,10),dpi=300)
plt.axis('off')
plt.imshow(wc, interpolation="bilinear")
plt.title('Wykop', color='#0054a2', fontsize=15)
plt.text(0.5, 0.0, 'wykop.pl', ha='center', va='center', fontsize=12, color='gray', transform=plt.gca().transAxes)
plt.show()
```


    
![svg](analiza_sentymentu_files/analiza_sentymentu_24_0.svg)
    


## Analiza sentymentu
Przechodzimy do w≈Ça≈õciwej analizy sentymentu.


```python
# Do≈ÇƒÖczenie s≈Çownika sentymentu
df_reddit = df_reddit.merge(
    slownik_sentymentu,
    left_on='lemma_clean', 
    right_on='lemat',
    how='left'
)
df_wykop = df_wykop.merge(
    slownik_sentymentu,
    left_on='lemma_clean', 
    right_on='lemat',
    how='left'
)

# Zliczenie ilo≈õci s≈Ç√≥w o danym ≈Çadunku emocjonalnym
sentyment_suma_reddit = df_reddit.groupby('nachechowanie').size().reset_index(name='ilosc')
sentyment_suma_wykop = df_wykop.groupby('nachechowanie').size().reset_index(name='ilosc')

# Pominiƒôcie s≈Ç√≥w bez przypisanego ≈Çadunku
sentyment_suma_reddit = sentyment_suma_reddit[sentyment_suma_reddit['nachechowanie'].notna()]
sentyment_suma_wykop = sentyment_suma_wykop[sentyment_suma_wykop['nachechowanie'].notna()]
```

Do≈ÇƒÖczamy s≈Çownik ≈Çadunk√≥w emocjonalnych do ostatecznie uzyskanej ramki danych i zliczamy wszystkie s≈Çowa o danym ≈Çadunku.


```python

# Ustalenie kolejno≈õci argument√≥w
custom_order = ['+ m', '+ s', 'amb', '- s', '- m']
sentyment_suma_reddit['nachechowanie'] = pd.Categorical(
    sentyment_suma_reddit['nachechowanie'],
    categories=custom_order,
    ordered=True
)
sentyment_suma_wykop['nachechowanie'] = pd.Categorical(
    sentyment_suma_wykop['nachechowanie'],
    categories=custom_order,
    ordered=True
)

sentyment_suma_reddit = sentyment_suma_reddit.sort_values('nachechowanie', ascending=False)
sentyment_suma_wykop = sentyment_suma_wykop.sort_values('nachechowanie', ascending=False)

# Mapa kolor√≥w dla skali emocji
color_map = {
    '+ m': '#2ecc40',   # Zielony
    '+ s': '#a3e048',   # Jasnozielony
    'amb': '#ffd700',   # ≈ª√≥≈Çty
    '- s': '#ff8c00',   # Pomara≈Ñczowy
    '- m': '#e74c3c'    # Czerwony
}
sentyment_suma_reddit['color'] = sentyment_suma_reddit['nachechowanie'].map(color_map)
sentyment_suma_wykop['color'] = sentyment_suma_wykop['nachechowanie'].map(color_map)

# Mapa opisowych nazw dla skali emocji
label_map = {
    '+ m': 'bardzo pozytywny',
    '+ s': 'pozytywny',
    'amb': 'niejednoznaczny',
    '- s': 'negatywny',
    '- m': 'bardzo negatywny'
}
```

 Rezultat zostaje przedstawiony na wykresie lizakowym za pomocƒÖ biblioteki `matplotlib`.


```python
# Nowa kolumna z opisowymi nazwami do wy≈õwietlenia na osi Y
sentyment_suma_reddit['label'] = sentyment_suma_reddit['nachechowanie'].map(label_map)
plt.figure(figsize=(10, 6))

# Wykres
for idx, row in sentyment_suma_reddit.iterrows():
    plt.hlines(
        y=row['label'],
        xmin=0,
        xmax=row['ilosc'],
        color=row['color'],
        linewidth=3
    )
    plt.plot(
        row['ilosc'],
        row['label'],
        'o',
        markersize=10,
        color=row['color']
    )
    plt.text(
        row['ilosc'] + max(sentyment_suma_reddit['ilosc']) * 0.02,
        row['label'],
        str(row['ilosc']),
        va='center',
        ha='left',
        fontsize=14,
        color='black'
    )

plt.title('Analiza sentymentu (Reddit)', fontsize=18, pad=15)
plt.xlabel('Liczba obserwacji', labelpad=15, fontsize = 14)
plt.ylabel('≈Åadunek emocjonalny', labelpad=15, fontsize = 14)
plt.grid(axis='x', linestyle='--', alpha=0.7)
max_ilosc = sentyment_suma_reddit['ilosc'].max()
plt.xlim(0, max_ilosc * 1.15)
plt.tight_layout()
plt.show()
```


    
![svg](analiza_sentymentu_files/analiza_sentymentu_30_0.svg)
    



```python
# Nowa kolumna z opisowymi nazwami do wy≈õwietlenia na osi Y
sentyment_suma_wykop['label'] = sentyment_suma_wykop['nachechowanie'].map(label_map)
plt.figure(figsize=(10, 6))

# Wykres
for idx, row in sentyment_suma_wykop.iterrows():
    plt.hlines(
        y=row['label'],
        xmin=0,
        xmax=row['ilosc'],
        color=row['color'],
        linewidth=3
    )
    plt.plot(
        row['ilosc'],
        row['label'],
        'o',
        markersize=10,
        color=row['color']
    )
    plt.text(
        row['ilosc'] + max(sentyment_suma_wykop['ilosc']) * 0.02,
        row['label'],
        str(row['ilosc']),
        va='center',
        ha='left',
        fontsize=14,
        color='black'
    )

plt.title('Analiza sentymentu (Wykop)', fontsize=18, pad=15)
plt.xlabel('Liczba obserwacji', labelpad=15, fontsize = 14)
plt.ylabel('≈Åadunek emocjonalny', labelpad=15, fontsize = 14)
plt.grid(axis='x', linestyle='--', alpha=0.7)
max_ilosc = sentyment_suma_reddit['ilosc'].max()
plt.xlim(0, max_ilosc * 1.15)
plt.tight_layout()
plt.show()
```


    
![svg](analiza_sentymentu_files/analiza_sentymentu_31_0.svg)
    


## WystƒÖpienia poszczeg√≥lnych kandydat√≥w
Teraz skupimy siƒô bezpo≈õrednio na kwestii obecnych wybor√≥w. Najpierw zliczymy ile razy wspomniano poszczeg√≥lnych kandydat√≥w, zliczajƒÖc wystƒÖpienia ich nazwisk odmienionych przez wszystkie przypadki

Minus tej metody: Internauci uwielbiajƒÖ nadawaƒá ksywki, kt√≥rych tutaj nie uwzglƒôdniamy.


```python
df_kandydaci_r = df_reddit[['slowa']]
slownik_kandydaci = pd.read_csv('s≈Çowniki/slownik_kandydaci.csv', sep=',')
df_kandydaci_r = df_kandydaci_r.merge(
    slownik_kandydaci,
    left_on='slowa', 
    right_on='odmieniony',
    how='left'
)

count_kandydaci_r = pd.DataFrame({'Kandydaci': ['Trzaskowski', 'Nawrocki', 'Mentzen', 'Ho≈Çownia','Biejat',
 'Zandberg','Braun','Stanowski','Senyszyn','Jakubiak','Bartoszewicz','Woch','Maciak']})
WystƒÖpienia = df_kandydaci_r['mianownik'].value_counts()
count_kandydaci_r['WystƒÖpienia'] = count_kandydaci_r['Kandydaci'].map(WystƒÖpienia).fillna(0).astype(int)
kandydaci_posortowani_r = count_kandydaci_r.sort_values(by='WystƒÖpienia', ascending=False)
#print('Reddit:')
#print(kandydaci_posortowani_r)

df_kandydaci_w = df_wykop[['slowa']]
slownik_kandydaci = pd.read_csv('s≈Çowniki/slownik_kandydaci.csv', sep=',')
df_kandydaci_w = df_kandydaci_w.merge(
    slownik_kandydaci,
    left_on='slowa', 
    right_on='odmieniony',
    how='left'
)

count_kandydaci_w = pd.DataFrame({'Kandydaci': ['Trzaskowski', 'Nawrocki', 'Mentzen', 'Ho≈Çownia','Biejat',
 'Zandberg','Braun','Stanowski','Senyszyn','Jakubiak','Bartoszewicz','Woch','Maciak']})
WystƒÖpienia = df_kandydaci_w['mianownik'].value_counts()
count_kandydaci_w['WystƒÖpienia'] = count_kandydaci_w['Kandydaci'].map(WystƒÖpienia).fillna(0).astype(int)
kandydaci_posortowani_w = count_kandydaci_w.sort_values(by='WystƒÖpienia', ascending=False)
#print('Wykop:')
#print(kandydaci_posortowani_w)
print(type(count_kandydaci_r))
```

    <class 'pandas.core.frame.DataFrame'>
    

## Wykres czƒôsto≈õci wystƒÖpie≈Ñ nazwisk kandydat√≥w


```python
# tu bƒôdzie kod wykresu
count_kandydaci_r.plot.bar(x='Kandydaci', y='WystƒÖpienia', rot=0, width=0.9, color = 'darkorange', 
title = "Ile razy wspomniano kandydat√≥w (Reddit)")
plt.xticks(rotation=70)                   
plt.tight_layout()                        
plt.show()
```


    
![svg](analiza_sentymentu_files/analiza_sentymentu_35_0.svg)
    



```python
count_kandydaci_w.plot.bar(x='Kandydaci', y='WystƒÖpienia', rot=0, width=0.9, color = 'blue', 
title = "Ile razy wspomniano kandydat√≥w (Wykop)")
plt.xticks(rotation=70)                   
plt.tight_layout()                        
plt.show()
```


    
![svg](analiza_sentymentu_files/analiza_sentymentu_36_0.svg)
    


## Asocjacje dla nazwisk kandydat√≥w
Tym razem tekst musi zostaƒá podzielony na ca≈Çe wypowiedzi, a nie s≈Çowa.


```python
import re
with open('data/results_reddit.txt', 'r', encoding='utf-8') as file:
    tekst_asoc_r = file.read()

# Usuwanie listy wyrazow
words_to_remove = ['r/Polska', 'polski', 'polska', 'kandydat', 'prezydent','ten≈ºe','wybory', 'oda','sonda']
pattern_words = r'\b(?:' + '|'.join(words_to_remove) + r')\b'

zmieniane = [
    # Zamiana "?" i "!" na kropki
    (r'[?!]', '.'),          
    # Usuwanie link√≥w i hashtag√≥w         
    (r'\bhttps?:\S*|#\w+|' + pattern_words, ''),
    # Usuwanie znak√≥w specjalnych i liczb       
    (r'[{}]|\d+'.format(re.escape("''\"*,.:;>[]()%|<+=")), ''),
    # Zamiana / na spacjƒô
    (r'/', ' '),
    # Usuwanie wielokrotnych spacji                      
    (r'\s+', ' ')                     
]
for pattern, replacement in zmieniane:
    tekst_asoc_r = re.sub(pattern, replacement, tekst_asoc_r, flags=re.IGNORECASE)

tekst_asoc_r = tekst_asoc_r.strip()
wypowiedz_asoc_r = re.split(r'\s*-{50}\s*', tekst_asoc_r)
```

Nastƒôpnie dokonujemy lematyzacji i usuniƒôcia stopwords z zachowaniem podzia≈Çu na zdania.


```python
# lematyzacja
morfeusz = morfeusz2.Morfeusz()

# Funkcja do lematyzacji pojedynczego s≈Çowa
def lemmatize(word: str) -> str:
    try:
        analysis = morfeusz.analyse(word)
        if analysis:
            return analysis[0][2][1].split(':')[0]
        return word
    except:
        return word

# Lematyzacja zda≈Ñ z zachowaniem struktury
asoc_lem_r = []
for wypowiedz in wypowiedz_asoc_r:
    # Dzielimy wypowiedz na s≈Çowa, lematyzujemy i ≈ÇƒÖczymy z powrotem
    lematyzowane_slowa = [lemmatize(slowo) for slowo in wypowiedz.split()]
    asoc_lem_r.append(' '.join(lematyzowane_slowa))

# Usuwanie stopwords
def usun_stopwords(sentence, stopwords):
    words = sentence.split()
    filtered_words = [word for word in words if word not in stopwords]
    return ' '.join(filtered_words)

asoc_lem_sw_r = [usun_stopwords(zdanie, df_polish_stop_words) for zdanie in asoc_lem_r]
#ostateczne wyczyszczenie z - i spacji
def clean_text_list(text_list):
    cleaned_list = []
    for text in text_list:
        text = re.sub(r'[‚Äû‚Äì‚Äù-]', '', text)
        text = re.sub(r'\s{2,}', ' ', text)
        text = text.strip()
        cleaned_list.append(text)
    return cleaned_list

asoc_lem_sw_c_r = clean_text_list(asoc_lem_sw_r)
# Zamiana na lowercase
asoc_gotowe_r = [zdanie.lower() for zdanie in asoc_lem_sw_c_r]
```

Macierz czƒôsto≈õci s≈Ç√≥w


```python
from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(asoc_gotowe_r)
df_asoc_r = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())
macierz_korelacji_r = df_asoc_r.corr(method='pearson')
# Funkcja znajdujƒÖca s≈Çowa najbardziej skorelowane z podanym s≈Çowem
def findAssocs(correlation_matrix, term, cor_limit=0.5):      
    if term not in correlation_matrix.columns:
        return f"S≈Çowo '{term}' nie wystƒôpuje"

    correlations = correlation_matrix[term]
    high_correlations = correlations[(correlations >= cor_limit) & 
                                   (correlations.index != term)]
    # Zwracane jest series z posortowanymi warto≈õciami korelacji powy≈ºej podanego progu
    return high_correlations.sort_values(ascending=False)
```

    <class 'pandas.core.series.Series'>
    

Wykresy asocjacji dla Reddita


```python
def asocjator1000(macierz, nazwisko, sila):
    # Pobierz korelacje dla danego s≈Çowa
    nazwisko_corr = findAssocs(macierz, nazwisko, sila)
   
    if isinstance(nazwisko_corr, str):
        print(nazwisko_corr)
        return
    if len(nazwisko_corr) == 0:
        print(f"Nie znaleziono s≈Ç√≥w z korelacjƒÖ wy≈ºszƒÖ ni≈º {sila} dla '{nazwisko}'")
        return
    bar_height = 0.35
    fig_height = len(nazwisko_corr) * bar_height + 1.5
    fig, ax = plt.subplots(figsize=(8, fig_height))
    bars = ax.barh(
        y=nazwisko_corr.index,
        width=nazwisko_corr.values,
        height=bar_height,           
        color='darkorange',
        edgecolor='darkred',
        alpha=0.8
    )
    for bar in bars:
        width = bar.get_width()
        ax.text(
            width + 0.01,
            bar.get_y() + bar.get_height()/2,
            f'{width:.3f}',
            va='center',
            fontsize=8
        )
   
    ax.grid(True, axis='x', linestyle='--', alpha=0.7)
    ax.invert_yaxis()
    ax.set_ylim(len(nazwisko_corr) - 0.5, -0.5)
    ax.tick_params(axis='y', labelsize=9)
    ax.tick_params(axis='x', labelsize=9)
    ax.set_title(f'Asocjacje dla: {nazwisko}', fontsize=12)
    ax.set_xlabel('Warto≈õƒá korelacji', fontsize=10)
    plt.tight_layout(pad=0.5)
# Trzaskowski
asocjator1000(macierz_korelacji_r,'trzaskowski',0.7)
# Nawrocki
asocjator1000(macierz_korelacji_r,'nawrocki',0.8)
# Mentzen
asocjator1000(macierz_korelacji_r,'mentzen',0.8)
# Ho≈Çownia
asocjator1000(macierz_korelacji_r,'ho≈Çownia',0.8)
# Biejat
asocjator1000(macierz_korelacji_r,'biejat',0.8)
# Zandberg
asocjator1000(macierz_korelacji_r,'zandberg',0.8)
# Braun
asocjator1000(macierz_korelacji_r,'braun',0.8)
# Stanowski
asocjator1000(macierz_korelacji_r,'stanowski',0.8)
# Senyszyn
asocjator1000(macierz_korelacji_r,'senyszyn',0.8)
# Jakubiak
asocjator1000(macierz_korelacji_r,'jakubiak',0.8)
# Bartoszewicz
asocjator1000(macierz_korelacji_r,'bartoszewicz',0.8)
# Maciak
asocjator1000(macierz_korelacji_r,'maciak',0.8)
# Woch
asocjator1000(macierz_korelacji_r,'woch',0.8)
```


    
![svg](analiza_sentymentu_files/analiza_sentymentu_44_0.svg)
    



    
![svg](analiza_sentymentu_files/analiza_sentymentu_44_1.svg)
    



    
![svg](analiza_sentymentu_files/analiza_sentymentu_44_2.svg)
    



    
![svg](analiza_sentymentu_files/analiza_sentymentu_44_3.svg)
    



    
![svg](analiza_sentymentu_files/analiza_sentymentu_44_4.svg)
    



    
![svg](analiza_sentymentu_files/analiza_sentymentu_44_5.svg)
    



    
![svg](analiza_sentymentu_files/analiza_sentymentu_44_6.svg)
    



    
![svg](analiza_sentymentu_files/analiza_sentymentu_44_7.svg)
    



    
![svg](analiza_sentymentu_files/analiza_sentymentu_44_8.svg)
    



    
![svg](analiza_sentymentu_files/analiza_sentymentu_44_9.svg)
    



    
![svg](analiza_sentymentu_files/analiza_sentymentu_44_10.svg)
    



    
![svg](analiza_sentymentu_files/analiza_sentymentu_44_11.svg)
    



    
![svg](analiza_sentymentu_files/analiza_sentymentu_44_12.svg)
    



```python

```
