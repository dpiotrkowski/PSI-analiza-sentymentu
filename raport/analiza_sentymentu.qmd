---
title: "ñ¶π Echo wyborcze"
author: 
  - name: "Celina Brzywczy"
    url: "https://github.com/CelinaBrz"
  - name: "Dawid Piotrkowski"
    url: "https://github.com/dpiotrkowski"
date: "2025-05-20"  # This will insert the current date
format:
  html:
    theme: journal
    linkcolor: "#367da9"
    toc: true
    toc-location: left
    self-contained: true
---

## Importowanie bibliotek
Biblioteka `morfeusz2` pos≈Çu≈ºy do lematyzacji, `plwordnet` i S≈Çowosieƒá do analizy sentymentu. 

>**S≈Çowosieƒá** (ang. PlWordNet) ‚Äì baza danych leksykalno-semantycznych jƒôzyka polskiego typu wordnet. Zawiera zestawy synonimicznych jednostek leksykalnych (synsety) opisanych kr√≥tkimi definicjami. S≈Çowosieƒá s≈Çu≈ºy jako s≈Çownik, w kt√≥rym pojƒôcia (synsety) i poszczeg√≥lne znaczenia wyraz√≥w (jednostki leksykalne) zdefiniowane sƒÖ poprzez miejsce w sieci wzajemnych relacji, odzwierciedlajƒÖcych system leksykalny polszczyzny. S≈Çowosieƒá jest tak≈ºe wykorzystywana jako jeden z podstawowych zasob√≥w do budowy program√≥w przetwarzajƒÖcych jƒôzyk polski. ([Wikipedia](https://pl.wikipedia.org/wiki/S%C5%82owosie%C4%87))


- http://morfeusz.sgjp.pl/download/
- https://slowosiec.pl/


```python
# import plwordnet
import morfeusz2
import pandas as pd
import numpy as np
```


```python
# biblioteki do wizualizacji danych
import matplotlib.pyplot as plt
from matplotlib import font_manager
from matplotlib_inline.backend_inline import set_matplotlib_formats
from wordcloud import WordCloud
from wordcloud import ImageColorGenerator
from PIL import Image

set_matplotlib_formats('')
```

## Opis danych
Dane pochodzƒÖ z Reddita i Wykopu, zosta≈Çy one zescrapowane przy uwzglƒôdnieniu nastƒôpujƒÖcych tag√≥w: 

> wybory,prezydent,prezydenckie,debata,prezydencka,
> Zandberg,Biejat,Nawrocki,Trzaskowski,Ho≈Çownia,Mentzen,Braun,
> Maciak,Woch,Jakubiak,Senyszyn,Bartoszewicz,Stanowski

[Nasza dokumentacja procesu webscrapingu](https://github.com/dpiotrkowski/PSI-analiza-sentymentu/blob/main/src/README.md)

![](/img/loga.png)

## Wstƒôpne przetwarzanie tekstu
W tym punkcie podejmiemy nastƒôpujƒÖce kroki:

1. Usuniƒôcie niepotrzebnych znak√≥w z tekstu ≈∫r√≥d≈Çowego
2. Autokorekta tekstu
3. Lematyzacja za pomocƒÖ Morfeusza
4. Usuniƒôcie stop s≈Ç√≥w z lemat√≥w
   
Na poczƒÖtku za≈Çadujmy polski s≈Çownik sentymentu (rƒôcznie przefiltrowany `plwordnet` w csv). 


```python
slownik_sentymentu = pd.read_csv('s≈Çowniki/S≈ÇownikSentymentu.csv', sep=';',encoding='cp1250')
# print(slownik_sentymentu.columns.tolist())
```

Wczytujemy zescrapowane pliki tekstowe z Reddita/Wykopu.


```python
with open('data/results_reddit.txt', 'r', encoding='utf-8') as file:
    reddit_messy = file.read()
with open('data/results_wykop.txt', 'r', encoding='utf-8') as file:
    wykop_messy = file.read()
```

Kolejnym krokiem bƒôdzie przeczyszczenie wczytanego tekstu, czyli:
- usuniƒôcie znak√≥w interpunkcyjnych
- usuniƒôcie niepo≈ºƒÖdanych s≈Ç√≥w z tekstu ≈∫r√≥d≈Çowego
- usuniƒôcie wszystkich link√≥w z wpis√≥w
- usuniƒôcie tag√≥w (`#` popularne na Wykopie)
- usuniƒôcie liczb
- usuniƒôcie polskich stop s≈Ç√≥w

### 1. Czyszczenie tekstu

Funkcja `text_roomba` dok≈Çadnie czy≈õci tekst oraz dzieli go na pojedyncze s≈Çowa.


```python
niechciane_slowa = [
    'r/', 'Tytu≈Ç', 'tytu≈Ç', 'Tre≈õƒá', 'tre≈õƒá', 'Tag', 'Subreddit', 'rpolska',
    'r/Polska', 'polski', 'polska', 'kandydat', 'prezydent', 'ten≈ºe', 'wybory', 'oda', 'sonda',
    'BRAK', 'TRE≈öCI'
]

def text_roomba(text, unwanted_words):
    # Znaki interpunkcyjne
    punctuation = '-,? .>:"/[]+=!%<*|‚Äì‚Äû( Õ°¬∞ Õú ñ Õ°¬∞)'    
    table = str.maketrans('', '', punctuation)
    
    words = text.split()
    # Wszystkie s≈Çowa z ma≈Çej litery
    cleaned_words = [word.translate(table).lower() for word in words]  
    # Filtrowanie slow
    filtered_words = [word for word in cleaned_words if word not in unwanted_words and word != '']

    # Usuwanie linkow
    filtered_words_no_links = [
        word for word in filtered_words
        if not (word.startswith('http') or word.startswith('www'))
    ]

    # Usuwanie tagow
    filtered_words_no_tags = [
        word for word in filtered_words_no_links
        if not word.startswith('#')
    ]
    # Usuwanie liczb
    filtered_words_final = [
        word for word in filtered_words_no_tags
        if not word.isdigit()
    ]
    return filtered_words_final
reddit_txt = text_roomba(reddit_messy, niechciane_slowa)
wykop_txt = text_roomba(wykop_messy, niechciane_slowa)
```

Na samym ko≈Ñcu tworzymy ramkƒô danych z przefiltrowanymi s≈Çowami, gdzie ka≈ºde s≈Çowo jest w osobnym wierszu.


```python
df_reddit = pd.DataFrame(reddit_txt, columns=['slowa'])
df_wykop = pd.DataFrame(wykop_txt, columns=['slowa'])
```

### 2. Autokorekta tekstu
(TBC)

### 3. Lematyzacja
O Morfeuszu tw√≥rcy piszƒÖ, ≈ºe w skr√≥cie to s≈Çownik morfologiczny.
Matura z polskiego ju≈º za nami zar√≥wno w wersji podstawowej, jak i rozszerzonej, a z Morfeusza korzystamy w wersji ze s≈Çownikiem gramatyki jƒôzyka polskiego - [SGJP](http://sgjp.pl/o-slowniku/#liczby). 

**Wa≈ºne pojƒôcia** ‚Äì czytamy dokumentacjƒô tw√≥rc√≥w:

> Celem has≈Çowania (lematyzacji) jest wskazanie dla ka≈ºdego s≈Çowa tekstowego opisujƒÖcej je jednostki s≈Çownika morfologicznego (leksemu). Jest to wiƒôc analiza morfologiczna (lub tagowanie) ograniczona tylko do czƒô≈õci informacji o formach ‚Äî do lemat√≥w.

> Przybli≈ºone has≈Çowanie polegajƒÖce na odciƒôciu ze s≈Ç√≥w czƒô≈õci zmieniajƒÖcej siƒô przy odmianie bywa nazywane stemowaniem. Metoda ta ma sens dla jƒôzyk√≥w o ograniczonej fleksji, ale dla polskiego daje wyniki wysoce niezadowalajƒÖce. W kontek≈õcie Morfeusza m√≥wimy wiƒôc o prawdziwym has≈Çowaniu.

([Analizator morfologiczny Morfeusz](http://morfeusz.sgjp.pl/doc/about/)) 


```python
morfeusz = morfeusz2.Morfeusz()

def lemmatize_word(word):
    analyses = morfeusz.analyse(word)
    if analyses:
        # analyses to lista krotek: (start, end, (lemma, tag), score)
        # Bierzemy pierwszƒÖ lematƒô z analizy
        lemma = analyses[0][2][1]  # [0] - pierwszy wynik, [2] - tuple (base, lemma, tag), [1] - lemma
        return lemma
    else:
        return word

df_reddit['lemma'] = df_reddit['slowa'].apply(lemmatize_word)
df_wykop['lemma'] = df_wykop['slowa'].apply(lemmatize_word)
```

Najczƒôstsze s≈Çowa


```python
df_reddit['lemma_clean'] = df_reddit['lemma'].apply(lambda x: x.split(':')[0])
lemma_counts_reddit = df_reddit['lemma_clean'].value_counts()

df_wykop['lemma_clean'] = df_wykop['lemma'].apply(lambda x: x.split(':')[0])
lemma_counts_wykop = df_wykop['lemma_clean'].value_counts()
```

### 4. Stops≈Çowa
W ko≈Ñcu mo≈ºemy przeczy≈õciƒá lematy ze zbƒôdnych s≈Ç√≥w - stopwords (siƒô, na, i, w, itp...). W pliku `stopwords-pl.txt` znajdujƒÖ siƒô wszystkie stops≈Çowa. Pochodzi on z https://github.com/stopwords-iso/stopwords-pl


```python
with open('s≈Çowniki/stopwords-pl.txt', 'r', encoding='utf-8') as file:
    df_polish_stop_words = file.read().splitlines()

df_reddit = df_reddit[~df_reddit['lemma_clean'].isin(df_polish_stop_words)]
word_freq_reddit = df_reddit['lemma_clean'].value_counts().head(125)

df_wykop = df_wykop[~df_wykop['lemma_clean'].isin(df_polish_stop_words)]
word_freq_wykop = df_wykop['lemma_clean'].value_counts().head(125)
```

## Chmura s≈Ç√≥w

Wykorzystamy teraz biblioteki do wizualizacji danych `matplotlib` i `worldcloud` do stworzenia chmury s≈Ç√≥w. W ramce danych `czestosci_slow` nadal wystƒôpujƒÖ s≈Çowa, kt√≥re mog≈Çyby zostaƒá usuniƒôte np. *prezydent*, *kandydat*, *wybory*.

TworzƒÖc chmurƒô s≈Ç√≥w za pomocƒÖ biblioteki [`wordcloud`](https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html) mo≈ºna dodaƒá stops≈Çowa, dodajƒÖc argument `stopwords`. Jednak nie zdecydowali≈õmy siƒô na to, poniewa≈º usuniƒôcie stops≈Ç√≥w przyda siƒô nam r√≥wnie≈º przy analizie sentymentu.



```python
filtered_word_freq_reddit = {word: freq for word, freq in word_freq_reddit.items() if word not in niechciane_slowa}
filtered_word_freq_wykop = {word: freq for word, freq in word_freq_wykop.items() if word not in niechciane_slowa}
```


```python
font_path = 'raport/img/RedditSans.ttf'
font_prop = font_manager.FontProperties(fname=font_path)
mask_path = 'raport/img/mask-reddit.png'  
mask = Image.open(mask_path)
mask = mask.convert("L")  # Convert to grayscale
mask = np.array(mask)

wc = WordCloud(font_path = font_path,
               mask=mask,
               width=1920, height=1080,
               background_color = "white",
               colormap = "magma",
               contour_width=12,
               contour_color='#ff4500')
wc.generate_from_frequencies(filtered_word_freq_reddit)

plt.figure(figsize=(15,12),dpi=300)
plt.axis('off')
plt.imshow(wc, interpolation="bilinear")
plt.title('Reddit', color='#ff4500', fontsize=15)
plt.text(0.5, 0.0, 'reddit.com/r/Polska', ha='center', va='center', fontsize=12, color='gray', transform=plt.gca().transAxes)
plt.show()
```


    
![](fig/analiza_sentymentu_23_0.svg)
    



```python
mask_path = 'raport/img/mask-wykop.png'  
mask = Image.open(mask_path)
mask = mask.convert("L")  # Convert to grayscale
mask = np.array(mask)


def orange_center_color_func(word, font_size, position, orientation, random_state=None, **kwargs):
    # Get the frequency of the word
    frequency = wc.words_.get(word, 0)
    
    # Assign orange to the most frequent words and blue to others
    if frequency > 0.3:  # Adjust this threshold based on your needs
        return "orange"
    else:
        return "blue"


wc = WordCloud(font_path = font_path,
               mask=mask,
               width=1920, height=1080,
               background_color = "white",
               color_func=orange_center_color_func,
               #colormap = "magma",
               contour_width=30,
               contour_color='#0054a2')
wc.generate_from_frequencies(filtered_word_freq_wykop)

plt.figure(figsize=(14,11),dpi=300)
plt.axis('off')
plt.imshow(wc, interpolation="bilinear")
plt.title('Wykop', color='#0054a2', fontsize=15)
plt.text(0.5, 0.0, 'wykop.pl', ha='center', va='center', fontsize=12, color='gray', transform=plt.gca().transAxes)
plt.show()
```


    
![](fig/analiza_sentymentu_24_0.svg)
    


## Analiza sentymentu
Przechodzimy do w≈Ça≈õciwej analizy sentymentu.


```python
# Do≈ÇƒÖczenie s≈Çownika sentymentu
df_reddit = df_reddit.merge(
    slownik_sentymentu,
    left_on='lemma_clean', 
    right_on='lemat',
    how='left'
)
df_wykop = df_wykop.merge(
    slownik_sentymentu,
    left_on='lemma_clean', 
    right_on='lemat',
    how='left'
)

# Zliczenie ilo≈õci s≈Ç√≥w o danym ≈Çadunku emocjonalnym
sentyment_suma_reddit = df_reddit.groupby('nachechowanie').size().reset_index(name='ilosc')
sentyment_suma_wykop = df_wykop.groupby('nachechowanie').size().reset_index(name='ilosc')

# Pominiƒôcie s≈Ç√≥w bez przypisanego ≈Çadunku
sentyment_suma_reddit = sentyment_suma_reddit[sentyment_suma_reddit['nachechowanie'].notna()]
sentyment_suma_wykop = sentyment_suma_wykop[sentyment_suma_wykop['nachechowanie'].notna()]
```

Do≈ÇƒÖczamy s≈Çownik ≈Çadunk√≥w emocjonalnych do ostatecznie uzyskanej ramki danych i zliczamy wszystkie s≈Çowa o danym ≈Çadunku.


```python
# Mapa kolor√≥w i opisowych nazw dla skali emocji
custom_order = ['+ m', '+ s', 'amb', '- s', '- m']
label_map = {
    #      KOLOR       SENTYMENT
    '+ m': ['#2ecc40', 'bardzo pozytywny'],
    '+ s': ['#a3e048', 'pozytywny'],
    'amb': ['#ffd700', 'niejednoznaczny'],
    '- s': ['#ff8c00', 'negatywny'],
    '- m': ['#e74c3c', 'bardzo negatywny']
}
```

 Rezultat zostaje przedstawiony na wykresie lizakowym za pomocƒÖ biblioteki `matplotlib`.


```python
def analizator_sentymentu(platforma): 
    sentyment_suma = globals()[f'sentyment_suma_{platforma}']
    # Sortowanie 
    sentyment_suma['nachechowanie'] = pd.Categorical(
        sentyment_suma['nachechowanie'],
        categories=custom_order,
        ordered=True
    )
    sentyment_suma = sentyment_suma.sort_values('nachechowanie', ascending=False)
    # Przypisanie kolor√≥w i nacechowania
    sentyment_suma['label'] = sentyment_suma['nachechowanie'].map(lambda x: label_map[x][1]) 
    sentyment_suma['color'] = sentyment_suma['nachechowanie'].map(lambda x: label_map[x][0])
    # Stworzenie wykresu
    plt.figure(figsize=(10, 6))
    for idx, row in sentyment_suma.iterrows():
        plt.hlines(
            y=row['label'],
            xmin=0,
            xmax=row['ilosc'],
            color=row['color'],
            linewidth=3
        )
        plt.plot(
            row['ilosc'],
            row['label'],
            'o',
            markersize=10,
            color=row['color']
        )
        plt.text(
            row['ilosc'] + max(sentyment_suma['ilosc']) * 0.02,
            row['label'],
            str(row['ilosc']),
            va='center',
            ha='left',
            fontsize=14,
            color='black'
        )  
    plt.title(f'Analiza sentymentu ({platforma.capitalize()})', fontsize=18, pad=15)
    plt.xlabel('Liczba obserwacji', labelpad=15, fontsize = 14)
    plt.ylabel('≈Åadunek emocjonalny', labelpad=15, fontsize = 14)
    plt.grid(axis='x', linestyle='--', alpha=0.7)
    max_ilosc = sentyment_suma['ilosc'].max()
    plt.xlim(0, max_ilosc * 1.15)
    plt.tight_layout()
    plt.show()
analizator_sentymentu("reddit")
analizator_sentymentu("wykop")
```


    
![](fig/analiza_sentymentu_30_0.svg)
    



    
![](fig/analiza_sentymentu_30_1.svg)
    


## WystƒÖpienia poszczeg√≥lnych kandydat√≥w
Teraz skupimy siƒô bezpo≈õrednio na kwestii obecnych wybor√≥w. Najpierw zliczymy ile razy wspomniano poszczeg√≥lnych kandydat√≥w, zliczajƒÖc wystƒÖpienia ich nazwisk odmienionych przez wszystkie przypadki

Minus tej metody: Internauci uwielbiajƒÖ nadawaƒá ksywki, kt√≥rych tutaj nie uwzglƒôdniamy.


```python
df_kandydaci_r = df_reddit[['slowa']]
slownik_kandydaci = pd.read_csv('s≈Çowniki/slownik_kandydaci.csv', sep=',')
df_kandydaci_r = df_kandydaci_r.merge(
    slownik_kandydaci,
    left_on='slowa', 
    right_on='odmieniony',
    how='left'
)

count_kandydaci_r = pd.DataFrame({'Kandydaci': ['Trzaskowski', 'Nawrocki', 'Mentzen', 'Ho≈Çownia','Biejat',
 'Zandberg','Braun','Stanowski','Senyszyn','Jakubiak','Bartoszewicz','Woch','Maciak']})
WystƒÖpienia = df_kandydaci_r['mianownik'].value_counts()
count_kandydaci_r['WystƒÖpienia'] = count_kandydaci_r['Kandydaci'].map(WystƒÖpienia).fillna(0).astype(int)
kandydaci_posortowani_r = count_kandydaci_r.sort_values(by='WystƒÖpienia', ascending=False)
#print('Reddit:')
#print(kandydaci_posortowani_r)

df_kandydaci_w = df_wykop[['slowa']]
slownik_kandydaci = pd.read_csv('s≈Çowniki/slownik_kandydaci.csv', sep=',')
df_kandydaci_w = df_kandydaci_w.merge(
    slownik_kandydaci,
    left_on='slowa', 
    right_on='odmieniony',
    how='left'
)

count_kandydaci_w = pd.DataFrame({'Kandydaci': ['Trzaskowski', 'Nawrocki', 'Mentzen', 'Ho≈Çownia','Biejat',
 'Zandberg','Braun','Stanowski','Senyszyn','Jakubiak','Bartoszewicz','Woch','Maciak']})
WystƒÖpienia = df_kandydaci_w['mianownik'].value_counts()
count_kandydaci_w['WystƒÖpienia'] = count_kandydaci_w['Kandydaci'].map(WystƒÖpienia).fillna(0).astype(int)
kandydaci_posortowani_w = count_kandydaci_w.sort_values(by='WystƒÖpienia', ascending=False)
#print('Wykop:')
#print(kandydaci_posortowani_w)
print(type(count_kandydaci_r))
```

    <class 'pandas.core.frame.DataFrame'>


## Wykres czƒôsto≈õci wystƒÖpie≈Ñ nazwisk kandydat√≥w


```python
# tu bƒôdzie kod wykresu
count_kandydaci_r.plot.bar(x='Kandydaci', y='WystƒÖpienia', rot=0, width=0.9, color = '#ff4500', 
title = "Ile razy wspomniano kandydat√≥w (Reddit)")
plt.xticks(rotation=70)                   
plt.tight_layout()                        
plt.show()
```


    
![](fig/analiza_sentymentu_34_0.svg)
    



```python
count_kandydaci_w.plot.bar(x='Kandydaci', y='WystƒÖpienia', rot=0, width=0.9, color = '#0054a2', 
title = "Ile razy wspomniano kandydat√≥w (Wykop)")
plt.xticks(rotation=70)                   
plt.tight_layout()                        
plt.show()
```


    
![](fig/analiza_sentymentu_35_0.svg)
    


## Asocjacje dla nazwisk kandydat√≥w
Tym razem tekst musi zostaƒá podzielony na ca≈Çe wypowiedzi, a nie s≈Çowa.


```python
import re
# Usuwanie listy wyrazow
pattern_words = r'\b(?:' + '|'.join(niechciane_slowa) + r')\b'

zmieniane = [
    # Zamiana "?" i "!" na kropki
    (r'[?!]', '.'),          
    # Usuwanie link√≥w i hashtag√≥w         
    (r'\bhttps?:\S*|#\w+|' + pattern_words, ''),
    # Usuwanie znak√≥w specjalnych i liczb       
    (r'[{}]|\d+'.format(re.escape("''\"*,.:;>[]()%|<+=")), ''),
    # Zamiana / na spacjƒô
    (r'/', ' '),
    # Usuwanie wielokrotnych spacji                      
    (r'\s+', ' ')                     
]
for pattern, replacement in zmieniane:
    reddit_messy = re.sub(pattern, replacement, reddit_messy, flags=re.IGNORECASE)

reddit_messy = reddit_messy.strip()

wypowiedz_asoc_r = re.split(r'\s*-{50}\s*', reddit_messy)
```

Nastƒôpnie dokonujemy lematyzacji i usuniƒôcia stopwords z zachowaniem podzia≈Çu na zdania.


```python
# Funkcja do lematyzacji pojedynczego s≈Çowa
def lemmatize(word: str) -> str:
    try:
        analysis = morfeusz.analyse(word)
        if analysis:
            return analysis[0][2][1].split(':')[0]
        return word
    except:
        return word

# Lematyzacja zda≈Ñ z zachowaniem struktury
asoc_lem_r = []
for wypowiedz in wypowiedz_asoc_r:
    # Dzielimy wypowiedz na s≈Çowa, lematyzujemy i ≈ÇƒÖczymy z powrotem
    lematyzowane_slowa = [lemmatize(slowo) for slowo in wypowiedz.split()]
    asoc_lem_r.append(' '.join(lematyzowane_slowa))

# Usuwanie stopwords
def usun_stopwords(sentence, stopwords):
    words = sentence.split()
    filtered_words = [word for word in words if word not in stopwords]
    return ' '.join(filtered_words)

asoc_lem_sw_r = [usun_stopwords(zdanie, df_polish_stop_words) for zdanie in asoc_lem_r]
#ostateczne wyczyszczenie z - i spacji
def clean_text_list(text_list):
    cleaned_list = []
    for text in text_list:
        text = re.sub(r'[‚Äû‚Äì‚Äù-]', '', text)
        text = re.sub(r'\s{2,}', ' ', text)
        text = text.strip()
        cleaned_list.append(text)
    return cleaned_list

asoc_lem_sw_c_r = clean_text_list(asoc_lem_sw_r)
# Zamiana na lowercase
asoc_gotowe_r = [zdanie.lower() for zdanie in asoc_lem_sw_c_r]
```

Macierz czƒôsto≈õci s≈Ç√≥w


```python
from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(asoc_gotowe_r)
df_asoc_r = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())
macierz_korelacji_r = df_asoc_r.corr(method='pearson')
# Funkcja znajdujƒÖca s≈Çowa najbardziej skorelowane z podanym s≈Çowem
def findAssocs(correlation_matrix, term, cor_limit=0.5):      
    if term not in correlation_matrix.columns:
        return f"S≈Çowo '{term}' nie wystƒôpuje"

    correlations = correlation_matrix[term]
    high_correlations = correlations[(correlations >= cor_limit) & 
                                   (correlations.index != term)]
    # Zwracane jest series z posortowanymi warto≈õciami korelacji powy≈ºej podanego progu
    return high_correlations.sort_values(ascending=False)
```

Wykresy asocjacji dla Reddita


```python
def asocjator1000(macierz, nazwisko, top_n):
    nazwisko_corr = findAssocs(macierz, nazwisko)

    # Ogranicz do `top_n` najwy≈ºszych korelacji
    nazwisko_corr = nazwisko_corr.nlargest(top_n)

    bar_height = 0.35
    fig_height = len(nazwisko_corr) * bar_height + 1.5
    fig, ax = plt.subplots(figsize=(8, fig_height))
    bars = ax.barh(
        y=nazwisko_corr.index,
        width=nazwisko_corr.values,
        height=bar_height,           
        color='#f58237',
        edgecolor='darkred',
        alpha=0.8
    )
    for bar in bars:
        width = bar.get_width()
        ax.text(
            width + 0.01,
            bar.get_y() + bar.get_height()/2,
            f'{width:.3f}',
            va='center',
            fontsize=8
        )
   
    ax.grid(True, axis='x', linestyle='--', alpha=0.7)
    ax.invert_yaxis()
    ax.set_ylim(len(nazwisko_corr) - 0.5, -0.5)
    ax.tick_params(axis='y', labelsize=9)
    ax.tick_params(axis='x', labelsize=9)
    ax.set_title(f'Top {top_n} asocjacji dla: {nazwisko} (Reddit)', fontsize=12)
    ax.set_xlabel('Warto≈õƒá korelacji', fontsize=10)
    plt.tight_layout(pad=0.5)

kandydaci = [nazwisko.lower() for nazwisko in count_kandydaci_r["Kandydaci"]]
asocjacje_top8 = [asocjator1000(macierz_korelacji_r, kandydat, 8) for kandydat in kandydaci]

```


    
![](fig/analiza_sentymentu_43_0.svg)
    



    
![](fig/analiza_sentymentu_43_1.svg)
    



    
![](fig/analiza_sentymentu_43_2.svg)
    



    
![](fig/analiza_sentymentu_43_3.svg)
    



    
![](fig/analiza_sentymentu_43_4.svg)
    



    
![](fig/analiza_sentymentu_43_5.svg)
    



    
![](fig/analiza_sentymentu_43_6.svg)
    



    
![](fig/analiza_sentymentu_43_7.svg)
    



    
![](fig/analiza_sentymentu_43_8.svg)
    



    
![](fig/analiza_sentymentu_43_9.svg)
    



    
![](fig/analiza_sentymentu_43_10.svg)
    



    
![](fig/analiza_sentymentu_43_11.svg)
    



    
![](fig/analiza_sentymentu_43_12.svg)
    

